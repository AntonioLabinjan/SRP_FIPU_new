[Running] python -u "c:\Users\Korisnik\Desktop\Skladista_rudarenje_podataka_projekt-master\Checkpoint4\main.py"
Pokrecemo Spark session...
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
25/05/05 20:05:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark session successfully started.
Spark version: 3.5.5
pokrecemo country_dim
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
pokrecemo date_dim
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
pokrecemo party_dim
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
pokrecemo election_dim
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
pokrecemo election_history_dim
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
pokrecemo person_dim
pokrecemo elections_fact
 Pokrecemo pipeline.py
pokrecemo run_loading.py
pokrenuli smo main.py
ovo je ok
usli smo u main
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
Starting data extraction
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL_App
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
path for csv trying to be found
Trying to start Spark session...
3
2
1
JAR file found on the specified path.
Starting Spark session with the following configuration:
App name: ETL Extract - CSV
JAR path: file:///C:/Users/Korisnik/Desktop/Skladista_rudarenje_podataka_projekt-master/Connectors/mysql-connector-j-9.2.0.jar
Spark session successfully started.
Spark version: 3.5.5
Data extraction completed
Starting data transformation

 Starting all transformations...

 [1] Transforming Country dimension...

[Stage 9:>                                                        (0 + 12) / 12]

[Stage 9:>                (0 + 12) / 12][Stage 11:>                 (0 + 0) / 1]
25/05/05 20:05:39 ERROR Executor: Exception in task 4.0 in stage 9.0 (TID 10)
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.EOFException
	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)
	... 24 more
25/05/05 20:05:39 ERROR Executor: Exception in task 11.0 in stage 9.0 (TID 17)
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.EOFException
	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)
	... 24 more

[Stage 9:>                (0 + 12) / 12][Stage 11:>                 (0 + 1) / 1]
25/05/05 20:05:39 ERROR TaskSetManager: Task 4 in stage 9.0 failed 1 times; aborting job
Greska pri brojanju redova: An error occurred while calling o145.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 9.0 failed 1 times, most recent failure: Lost task 4.0 in stage 9.0 (TID 10) (pc executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 24 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 24 more



[Stage 9:>                                                        (0 + 10) / 12]

[Stage 9:>                 (0 + 9) / 12][Stage 13:>                (0 + 3) / 12]

[Stage 9:>   (0 + 9) / 12][Stage 13:>  (0 + 3) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 9:>   (0 + 8) / 12][Stage 13:>  (0 + 4) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 9:>   (0 + 7) / 12][Stage 13:>  (0 + 5) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 9:>   (0 + 6) / 12][Stage 13:>  (0 + 6) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 9:>   (0 + 5) / 12][Stage 13:>  (0 + 7) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 9:>   (0 + 4) / 12][Stage 13:>  (0 + 8) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 9:>   (0 + 3) / 12][Stage 13:>  (0 + 9) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 9:>   (0 + 2) / 12][Stage 13:> (0 + 10) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 9:>   (0 + 1) / 12][Stage 13:> (0 + 11) / 12][Stage 15:>   (0 + 0) / 1]

[Stage 13:>               (0 + 12) / 12][Stage 15:>                 (0 + 0) / 1]
25/05/05 20:05:52 ERROR Executor: Exception in task 10.0 in stage 13.0 (TID 30)
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.EOFException
	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)
	... 24 more
25/05/05 20:05:52 ERROR TaskSetManager: Task 10 in stage 13.0 failed 1 times; aborting job
 [1] Country dimension failed: An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 13.0 failed 1 times, most recent failure: Lost task 10.0 in stage 13.0 (TID 30) (pc executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 24 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 24 more


Traceback (most recent call last):
  File "c:\Users\Korisnik\Desktop\Skladista_rudarenje_podataka_projekt-master\Checkpoint4\transform\dimensions\country_dim.py", line 70, in transform_country_dim
    print("Broj redaka u dimenziji drzava:", final_df.count())
                                             ^^^^^^^^^^^^^^^^
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\pyspark\sql\dataframe.py", line 1240, in count

[Stage 13:>                                                       (0 + 11) / 12]
    return int(self._jdf.count())
               ^^^^^^^^^^^^^^^^^
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o145.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 9.0 failed 1 times, most recent failure: Lost task 4.0 in stage 9.0 (TID 10) (pc executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 24 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 24 more



During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\Korisnik\Desktop\Skladista_rudarenje_podataka_projekt-master\Checkpoint4\main.py", line 47, in <module>
    main()
  File "c:\Users\Korisnik\Desktop\Skladista_rudarenje_podataka_projekt-master\Checkpoint4\main.py", line 37, in main
    load_ready_dict = run_transformations(merged_df)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Korisnik\Desktop\Skladista_rudarenje_podataka_projekt-master\Checkpoint4\transform\pipeline.py", line 17, in run_transformations
    country_dim = transform_country_dim(
                  ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Korisnik\Desktop\Skladista_rudarenje_podataka_projekt-master\Checkpoint4\transform\dimensions\country_dim.py", line 73, in transform_country_dim
    final_df.show(5)
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\pyspark\sql\dataframe.py", line 947, in show
    print(self._show_string(n, truncate, vertical))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\pyspark\sql\dataframe.py", line 965, in _show_string
    return self._jdf.showString(n, 20, vertical)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\Users\Korisnik\AppData\Roaming\Python\Python312\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o145.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 13.0 failed 1 times, most recent failure: Lost task 10.0 in stage 13.0 (TID 30) (pc executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 24 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)

	at java.base/java.lang.Thread.run(Thread.java:834)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 24 more


25/05/05 20:05:52 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Korisnik\AppData\Local\Temp\spark-bf9135c4-e83d-436f-b617-5a512bbbcf36
java.io.IOException: Failed to delete: C:\Users\Korisnik\AppData\Local\Temp\spark-bf9135c4-e83d-436f-b617-5a512bbbcf36\userFiles-62606d31-cb88-4a17-82bd-658eadc4161e\mysql-connector-j-9.2.0.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:147)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
25/05/05 20:05:52 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Korisnik\AppData\Local\Temp\spark-bf9135c4-e83d-436f-b617-5a512bbbcf36\userFiles-62606d31-cb88-4a17-82bd-658eadc4161e
java.io.IOException: Failed to delete: C:\Users\Korisnik\AppData\Local\Temp\spark-bf9135c4-e83d-436f-b617-5a512bbbcf36\userFiles-62606d31-cb88-4a17-82bd-658eadc4161e\mysql-connector-j-9.2.0.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:147)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
SUCCESS: The process with PID 60276 (child process of PID 56504) has been terminated.
SUCCESS: The process with PID 56504 (child process of PID 47516) has been terminated.
SUCCESS: The process with PID 47516 (child process of PID 62320) has been terminated.
SUCCESS: The process with PID 62320 (child process of PID 58088) has been terminated.
SUCCESS: The process with PID 58088 (child process of PID 62260) has been terminated.

[Done] exited with code=1 in 73.232 seconds

